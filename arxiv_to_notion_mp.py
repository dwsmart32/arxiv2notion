import os
import json
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from google import genai
import time
from google.genai import types
import httpx
import re

# --- ì„¤ì • (Secrets) ---
NOTION_TOKEN = os.environ.get("NOTION_TOKEN")
DATABASE_ID = os.environ.get("DATABASE_ID_MP")
GOOGLE_API_KEY = os.environ.get("GOOGLE_API_KEY")
S2_API_KEY = os.environ.get("SEMANTICSCHOLAR_API_KEY") # âœ… [S2 ì¶”ê°€]

# --- ì„¤ì • (í‚¤ì›Œë“œ ë° í•„í„°) ---
BASE_KEYWORDS = [
    "Multi Party",
    "Multi Party Dialogues",
    "Multi speaker",
    "Multi speakers"
]

# âœ… [S2 í†µí•©] arXivì™€ S2ì˜ ì¹´í…Œê³ ë¦¬ ì´ë¦„ì´ ë‹¤ë¥´ë¯€ë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤.
ARXIV_ALLOWED_SUBJECTS = {"cs.CL", "cs.AI", "cs.LG", "cs.SD"}
S2_ALLOWED_SUBJECTS = {"Computer Science", "Linguistics", "Engineering"} # âœ… [S2 ì¶”ê°€]

MY_RESEARCH_AREA = "My research focuses on developing full duplex spoken language model that understands the multi-party conversation and situations"
LOOKBACK_DAYS = 360

# --- ê¸°ë³¸ ì²´í¬ ---
missing = [name for name, val in {
    "NOTION_TOKEN": NOTION_TOKEN,
    "DATABASE_ID_MP": DATABASE_ID,
    "GOOGLE_API_KEY": GOOGLE_API_KEY,
    "SEMANTICSCHOLAR_API_KEY": S2_API_KEY
}.items() if not val]

if missing:
    raise ValueError(f"âŒ ë‹¤ìŒ í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {', '.join(missing)}")
    
MODEL_LIST = ["gemini-1.5-pro-latest", "gemini-1.5-flash-latest", "gemini-pro"] # âœ… ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ ìµœì‹ í™”
current_model_index = 0

today = datetime.today()
lookback_date_obj = today - timedelta(days=LOOKBACK_DAYS) # âœ… [S2 í†µí•©] ë‚ ì§œ ê°ì²´ë¡œ ì €ì¥

# --- Gemini í´ë¼ì´ì–¸íŠ¸ ì„¤ì • ---
client = genai.Client(api_key=GOOGLE_API_KEY)


# --- í‚¤ì›Œë“œ í™•ì¥ í•¨ìˆ˜ ---
def expand_keywords(base_keywords):
    """
    ê¸°ë³¸ í‚¤ì›Œë“œ ëª©ë¡ì„ ë°›ì•„ ë‹¤ì–‘í•œ ë³€í˜•(í•˜ì´í”ˆ, ëŒ€ì†Œë¬¸ì)ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    expanded = set()
    for keyword in base_keywords:
        variants = {keyword}
        if ' ' in keyword:
            variants.add(keyword.replace(' ', '-'))
        if '-' in keyword:
            variants.add(keyword.replace('-', ' '))

        for variant in variants:
            expanded.add(variant.lower())
            expanded.add(variant.upper())
            expanded.add(variant.title())
            
    return list(expanded)

# âœ… [S2 í†µí•©] ìµœì¢… ê²€ìƒ‰ í‚¤ì›Œë“œ ëª©ë¡
KEYWORDS = expand_keywords(BASE_KEYWORDS)


# --- Notion DB í•¨ìˆ˜ ---
def fetch_existing_titles():
    """Notion ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê¸°ì¡´ ë…¼ë¬¸ ì œëª©ë“¤ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    url = f"https://api.notion.com/v1/databases/{DATABASE_ID}/query"
    headers = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Notion-Version": "2022-06-28",
        "Content-Type": "application/json"
    }
    titles = set()
    has_more = True
    next_cursor = None
    while has_more:
        data = {"start_cursor": next_cursor} if next_cursor else {}
        try:
            res = requests.post(url, headers=headers, json=data, timeout=10)
            res.raise_for_status()
            results = res.json()
            for page in results["results"]:
                try:
                    title = ' '.join(page["properties"]["Paper"]["title"][0]["text"]["content"].split())
                    titles.add(title)
                except (KeyError, IndexError):
                    continue
            has_more = results.get("has_more", False)
            next_cursor = results.get("next_cursor")
        except requests.exceptions.RequestException as e:
            print(f"âŒ Notion ì œëª© ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            break
    return titles

# --- ArXiv ë…¼ë¬¸ ìˆ˜ì§‘ í•¨ìˆ˜ ---
def fetch_arxiv_papers(lookback_date):
    """í‚¤ì›Œë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ arXivì—ì„œ ë…¼ë¬¸ì„ ê²€ìƒ‰í•˜ê³  ë‚ ì§œì™€ ì¹´í…Œê³ ë¦¬ë¡œ í•„í„°ë§í•©ë‹ˆë‹¤."""
    base_url = "http://export.arxiv.org/api/query?"
    unique_papers = {}
    print("â¬‡ï¸  [ArXiv] í‚¤ì›Œë“œ ê¸°ë°˜ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
    print(f"ğŸ’¡ ì´ {len(KEYWORDS)}ê°œì˜ í™•ì¥ëœ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤: {KEYWORDS}")
    
    today_date = datetime.today().date()

    for keyword in set(KEYWORDS):
        print(f"ğŸ” [ArXiv] í‚¤ì›Œë“œ ê²€ìƒ‰ ì¤‘: \"{keyword}\"")
        search_query = f'ti:"{keyword}" OR abs:"{keyword}"'
        params = f"search_query={search_query}&sortBy=submittedDate&sortOrder=descending&max_results=50"
        try:
            response = requests.get(base_url + params, timeout=10)
            response.raise_for_status()
        except requests.exceptions.RequestException as e:
            print(f"âŒ \"{keyword}\" ê²€ìƒ‰ ì¤‘ arXiv API ì˜¤ë¥˜: {e}")
            continue
        
        soup = BeautifulSoup(response.content, 'xml')
        entries = soup.find_all('entry')
        
        for entry in entries:
            # --- ë‚ ì§œ í•„í„°ë§ (ArXiv) ---
            updated_str = entry.updated.text
            updated_date = datetime.strptime(updated_str, "%Y-%m-%dT%H:%M:%SZ").date()
            if not (lookback_date <= updated_date <= today_date):
                continue

            # --- ì¹´í…Œê³ ë¦¬ í•„í„°ë§ (ArXiv) ---
            categories = [cat['term'] for cat in entry.find_all('category')]
            if not any(subject in categories for subject in ARXIV_ALLOWED_SUBJECTS):
                continue

            paper_abs_url = entry.id.text.strip()
            if paper_abs_url not in unique_papers:
                pdf_link_tag = entry.find('link', attrs={'title': 'pdf'})
                if pdf_link_tag and pdf_link_tag.get('href'):
                    paper_pdf_url = pdf_link_tag['href']
                else:
                    abs_https = paper_abs_url.replace('http://', 'https://')
                    paper_pdf_url = abs_https.replace('/abs/', '/pdf/')
                    if not paper_pdf_url.endswith('.pdf'):
                        paper_pdf_url += '.pdf'
                
                unique_papers[paper_abs_url] = {
                    'title': ' '.join(entry.title.text.strip().split()),
                    'link': paper_abs_url.replace('http://', 'https://'),
                    'pdf_link': paper_pdf_url,
                    'updated_str': updated_str, # ArXivëŠ” ì´ë¯¸ ISO í˜•ì‹ì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                    'abstract': ' '.join(entry.summary.text.strip().split()),
                    'author': entry.author.find('name').text.strip() if entry.author else 'arXiv',
                    'categories': categories
                }
        time.sleep(1)
        
    print(f"ğŸ‘ [ArXiv] ì´ {len(unique_papers)}ê°œì˜ ê³ ìœ  ë…¼ë¬¸ ë°œê²¬.")
    return list(unique_papers.values())


# --- âœ… [S2 ì¶”ê°€] Semantic Scholar ë…¼ë¬¸ ìˆ˜ì§‘ í•¨ìˆ˜ ---
def fetch_semantic_scholar_papers(keywords, lookback_date):
    """
    í‚¤ì›Œë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ Semantic Scholarì—ì„œ ë…¼ë¬¸ì„ ê²€ìƒ‰í•˜ê³ 
    ë‚ ì§œì™€ ì¹´í…Œê³ ë¦¬ë¡œ í•„í„°ë§í•˜ì—¬ 'í‘œì¤€ í˜•ì‹'ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    base_url = "https://api.semanticscholar.org/graph/v1/paper/search"
    
    s2_fields = [
        "paperId", "url", "title", "abstract", "authors",
        "publicationDate", "openAccessPdf", "fieldsOfStudy"
    ]
    
    headers = {'X-API-KEY': S2_API_KEY}
    unique_papers = {}
    today_date = datetime.today().date()

    print(f"â¬‡ï¸  [S2] Semantic Scholar ë…¼ë¬¸ ê²€ìƒ‰ ì‹œì‘ (ìµœê·¼ {LOOKBACK_DAYS}ì¼)...")

    for keyword in set(keywords):
        print(f"ğŸ” [S2] í‚¤ì›Œë“œ ê²€ìƒ‰ ì¤‘: \"{keyword}\"")
        
        params = {
            'query': keyword,
            'fields': ','.join(s2_fields),
            'sort': 'publicationDate:desc',
            'limit': 50
        }
        
        try:
            response = requests.get(base_url, headers=headers, params=params, timeout=15)
            response.raise_for_status()
            results = response.json()

            for paper_data in results.get('data', []):
                paper_id = paper_data.get('paperId')
                if not paper_id or paper_id in unique_papers:
                    continue

                # --- 1. ë‚ ì§œ í•„í„°ë§ (S2) ---
                pub_date_str = paper_data.get('publicationDate')
                if not pub_date_str:
                    continue
                
                try:
                    pub_date = datetime.strptime(pub_date_str, "%Y-%m-%d").date()
                except ValueError:
                    continue 

                if not (lookback_date <= pub_date <= today_date):
                    continue

                # --- 2. ì¹´í…Œê³ ë¦¬ í•„í„°ë§ (S2) ---
                categories = paper_data.get('fieldsOfStudy') or []
                if not categories or not any(subject in S2_ALLOWED_SUBJECTS for subject in categories):
                    continue
                
                # --- 3. í‘œì¤€ í˜•ì‹ìœ¼ë¡œ íŒŒì‹± (S2) ---
                authors_list = paper_data.get('authors', [])
                author_str = authors_list[0].get('name', 'S2') if authors_list else 'S2'
                
                oa_pdf = paper_data.get('openAccessPdf')
                pdf_link = oa_pdf.get('url') if (oa_pdf and oa_pdf.get('url')) else paper_data.get('url')

                # Notion ì €ì¥ì„ ìœ„í•´ ISO T Z í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                updated_str_iso = f"{pub_date_str}T00:00:00Z"

                # âœ… [ìˆ˜ì •] .get()ìœ¼ë¡œ ê°€ì ¸ì˜¨ ê°’ì´ Noneì¼ ê²½ìš°(JSON: null) 'or'ë¡œ ê¸°ë³¸ê°’ ì„¤ì •
                title_raw = paper_data.get('title')
                abstract_raw = paper_data.get('abstract')

                unique_papers[paper_id] = {
                    'title': ' '.join((title_raw or 'No Title').split()),
                    'link': paper_data.get('url'),
                    'pdf_link': pdf_link,
                    'updated_str': updated_str_iso,
                    'abstract': ' '.join((abstract_raw or 'N/A').split()),
                    'author': author_str,
                    'categories': categories
                }

        except requests.exceptions.RequestException as e:
            print(f"âŒ \"{keyword}\" ê²€ìƒ‰ ì¤‘ S2 API ì˜¤ë¥˜: {e}")
            continue
        
        time.sleep(1) # API ì†ë„ ì œí•œ ì¤€ìˆ˜

    print(f"ğŸ‘ [S2] ì´ {len(unique_papers)}ê°œì˜ ê³ ìœ  ë…¼ë¬¸ ë°œê²¬.")
    return list(unique_papers.values())

# --- Gemini ë¶„ì„ í•¨ìˆ˜ ---
def analyze_paper_with_gemini(paper):
    """
    Geminië¥¼ ì‚¬ìš©í•˜ì—¬ PDF ë…¼ë¬¸ì„ ë¶„ì„í•˜ê³ , ìš”ì•½ì„ 5ê°œ í•­ëª©ìœ¼ë¡œ íŒŒì‹±í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    global current_model_index

    # --- PDF ë‹¤ìš´ë¡œë“œ ---
    try:
        print(f"  - PDF ë‹¤ìš´ë¡œë“œ ì¤‘: {paper['pdf_link']}")
        headers = {"User-Agent": "paper-bot/1.0 (+github.com/dongwook-lee)"} # User-Agent ëª…ì‹œ
        
        # httpxë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ ìë™ ì²˜ë¦¬
        with httpx.Client(follow_redirects=True, timeout=30) as http_client:
             doc_response = http_client.get(paper['pdf_link'], headers=headers)
             doc_response.raise_for_status()
             doc_data = doc_response.content
        print("  - PDF ë‹¤ìš´ë¡œë“œ ì™„ë£Œ.")
    except (httpx.RequestError, httpx.HTTPStatusError) as e:
        print(f"  âŒ PDF ë‹¤ìš´ë¡œë“œ/ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return None, None

    # --- Gemini í”„ë¡¬í”„íŠ¸ (í•­ëª©ë³„ íƒœê·¸ ì¶”ê°€) ---
    prompt = f"""
    You are an AI assistant helping a researcher. Your task is to analyze the attached PDF paper and provide two outputs: an English summary divided into five specific sections, and an assessment of its relevance.

    **My Research Area:**
    "{MY_RESEARCH_AREA}"

    **Instructions:**

    1.  **Paper Summary (English):** Please summarize the paper, strictly following the five-part structure below. Use the exact tags `[MOTIVATION]`, `[DIFFERENCES]`, `[CONTRIBUTIONS]`, `[METHOD]`, `[RESULTS]` to label each section. Each section should be a concise paragraph.
        * `[MOTIVATION]`: What problem does this research aim to solve, and why is it important?
        * `[DIFFERENCES]`: How is this work different from or improving upon previous approaches?
        * `[CONTRIBUTIONS]`: What are the main contributions and novel aspects of this paper?
        * `[METHOD]`: What method or approach do the authors propose?
        * `[RESULTS]`: What are the key results that demonstrate the effectiveness of the proposed method?

    2.  **Relevance Assessment:** Please determine if the paperâ€™s contributions are directly relevant to my research area.

    3.  **Output Format:** You **MUST** follow the exact format below, using "|||" as a delimiter. Do not include any additional commentary or greetings.

    **Output Format:**
    [MOTIVATION]
    ... summary ...
    [DIFFERENCES]
    ... summary ...
    [CONTRIBUTIONS]
    ... summary ...
    [METHOD]
    ... summary ...
    [RESULTS]
    ... summary ...
    |||[Yes. or No.]
    """

    while current_model_index < len(MODEL_LIST):
        model_to_use = MODEL_LIST[current_model_index]
        print(f"  - Gemini ë¶„ì„ ì‹œë„ (ëª¨ë¸: {model_to_use})")
        
        try:
            # âœ… [S2 í†µí•©] ìµœì‹  Gemini API í˜¸ì¶œ ë°©ì‹ (genai.Client)
            response = client.models.generate_content(
                model=model_to_use,
                contents=[
                    types.Part.from_bytes(data=doc_data, mime_type='application/pdf'),
                    prompt
                ],
            )

            if response.text and '|||' in response.text:
                summary_part, answer_part = [p.strip() for p in response.text.strip().split('|||', 1)]
                
                # --- ì •ê·œí‘œí˜„ì‹ì„ ì´ìš©í•œ íŒŒì‹± ---
                tags = ["MOTIVATION", "DIFFERENCES", "CONTRIBUTIONS", "METHOD", "RESULTS"]
                parsed_summary = {}
                for i in range(len(tags)):
                    current_tag = tags[i]
                    next_tag = tags[i+1] if i + 1 < len(tags) else None
                    
                    pattern = f"\[{current_tag}\](.*?)"
                    if next_tag:
                        pattern = f"\[{current_tag}\](.*?)(?=\[{next_tag}\])"
                    else:
                        pattern = f"\[{current_tag}\](.*)"
                    
                    match = re.search(pattern, summary_part, re.DOTALL | re.IGNORECASE)
                    
                    if match:
                        content = match.group(1).strip()
                        # Notionì˜ í…ìŠ¤íŠ¸ í•„ë“œ ìµœëŒ€ ê¸¸ì´ëŠ” 2000ìì…ë‹ˆë‹¤.
                        parsed_summary[current_tag] = content[:1990] + '...' if len(content) > 2000 else content
                    else:
                        parsed_summary[current_tag] = "N/A"

                if all(tag in parsed_summary for tag in tags):
                    relevance = "Related" if "yes" in answer_part.lower() else "Unrelated"
                    return relevance, parsed_summary
            
            print(f"  âš ï¸ Geminiê°€ ì˜ˆìƒì¹˜ ëª»í•œ í˜•ì‹ìœ¼ë¡œ ë‹µë³€: {response.text[:200]}...")
            return None, None

        except Exception as e:
            if "overload" in str(e).lower():
                print(f"  â³ ëª¨ë¸ '{model_to_use}' ê³¼ë¶€í•˜. 30ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤.")
                time.sleep(30)
                continue
            else:
                if "resource_exhausted" in str(e).lower() or "quota" in str(e).lower():
                    print(f"  âš ï¸ ëª¨ë¸ '{model_to_use}'ì˜ API ì¿¼í„° ì†Œì§„. ë‹¤ìŒ ëª¨ë¸ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                    current_model_index += 1
                    time.sleep(2)
                else:
                    print(f"  âŒ Gemini API í˜¸ì¶œ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    return None, None

    print("  âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  Gemini ëª¨ë¸ì˜ ì¿¼í„°ë¥¼ ì†Œì§„í–ˆìŠµë‹ˆë‹¤.")
    return None, None

# --- Notion ì¶”ê°€ í•¨ìˆ˜ ---
def add_to_notion(paper, related_status, summary_parts):
    """ë…¼ë¬¸ ì •ë³´, ê´€ë ¨ë„, ë¶„í• ëœ ìš”ì•½ì„ Notionì— ì¶”ê°€í•©ë‹ˆë‹¤."""
    url = "https://api.notion.com/v1/pages"
    headers = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Content-Type": "application/json",
        "Notion-Version": "2022-06-28"
    }

    updated_str = paper['updated_str'].split('T')[0]

    properties = {
        "Paper": {"title": [{"text": {"content": paper['title']}}]},
        "Abstract": {"rich_text": [{"text": {"content": paper.get('abstract', 'N/A')[:1999]}}]}, # ì›ë³¸ ì´ˆë¡ (ê¸¸ì´ ì œí•œ)
        "Author": {"rich_text": [{"text": {"content": paper.get('author', 'N/A')}}]},
        "Relatedness": {"select": {"name": related_status}},
        "URL": {"url": paper['link']},
        "Date": {"date": {"start": updated_str}},
        "Motivation": {"rich_text": [{"text": {"content": summary_parts.get('MOTIVATION', 'N/A')}}]},
        "Differences from Prior Work": {"rich_text": [{"text": {"content": summary_parts.get('DIFFERENCES', 'N/A')}}]},
        "Contributions and Novelty": {"rich_text": [{"text": {"content": summary_parts.get('CONTRIBUTIONS', 'N/A')}}]},
        "Proposed Method": {"rich_text": [{"text": {"content": summary_parts.get('METHOD', 'N/A')}}]},
        "Results": {"rich_text": [{"text": {"content": summary_parts.get('RESULTS', 'N/A')}}]}
    }

    data = {"parent": {"database_id": DATABASE_ID}, "properties": properties}

    try:
        res = requests.post(url, headers=headers, json=data, timeout=15)
        if res.status_code == 200:
            print(f"âœ… Notion ë“±ë¡ ì„±ê³µ: {paper['title'][:60]}... (ìƒíƒœ: {related_status})")
        else:
            print(f"âŒ Notion ë“±ë¡ ì‹¤íŒ¨: {paper['title'][:60]}...")
            print(f"ğŸ“„ Notion ì‘ë‹µ: {res.status_code}")
            print(res.text)
    except requests.exceptions.RequestException as e:
        print(f"âŒ Notion API ìš”ì²­ ì‹¤íŒ¨: {paper['title'][:60]}... | {e}")


# --- ğŸš€ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ ---
def main():
    """ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ë…¼ë¬¸ ìë™í™” ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. (ArXiv + Semantic Scholar)")
    
    # --- âœ… [S2 í†µí•©] ë‚ ì§œ ê°ì²´ë¥¼ í•¨ìˆ˜ì— ì „ë‹¬í•˜ë„ë¡ ìˆ˜ì • ---
    lookback_date = lookback_date_obj.date()

    print("\n[1/5] ğŸ“š Notion DBì—ì„œ ê¸°ì¡´ ë…¼ë¬¸ ëª©ë¡ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
    existing_titles_lower = {title.lower() for title in fetch_existing_titles()}
    print(f"ì´ {len(existing_titles_lower)}ê°œì˜ ë…¼ë¬¸ì´ Notionì— ì¡´ì¬í•©ë‹ˆë‹¤.")

    print("\n[2/5] ğŸ” ë…¼ë¬¸ ìˆ˜ì§‘ ì¤‘...")
    # --- âœ… [S2 í†µí•©] ë‘ ì†ŒìŠ¤ì—ì„œ ëª¨ë‘ ë…¼ë¬¸ì„ ê°€ì ¸ì˜µë‹ˆë‹¤. ---
    arxiv_papers = fetch_arxiv_papers(lookback_date)
    s2_papers = fetch_semantic_scholar_papers(KEYWORDS, lookback_date)
    
    all_papers_raw = arxiv_papers + s2_papers
    print(f"--- \nâ¡ï¸  ì´ {len(all_papers_raw)}ê°œ ë…¼ë¬¸ ë°œê²¬ (ArXiv: {len(arxiv_papers)}, S2: {len(s2_papers)})")

    # --- âœ… [S2 í†µí•©] (ì¤‘ìš”) S2ì™€ ArXivì˜ ì¤‘ë³µì„ ì œëª© ê¸°ì¤€ìœ¼ë¡œ ì œê±°í•©ë‹ˆë‹¤. ---
    print("\n[3/5] ğŸ”„ (ArXiv + S2) í†µí•© ë¦¬ìŠ¤íŠ¸ ì¤‘ë³µ ì œê±° ì¤‘...")
    unique_papers_dict = {}
    for paper in all_papers_raw:
        title_lower = paper['title'].lower()
        if title_lower not in unique_papers_dict:
            unique_papers_dict[title_lower] = paper
    
    all_papers_filtered = list(unique_papers_dict.values())
    print(f"ğŸ‘ ì¤‘ë³µ ì œê±° í›„ ì´ {len(all_papers_filtered)}ê°œì˜ ê³ ìœ  ë…¼ë¬¸ í™•ë³´.")

    analyzed_papers = []
    if all_papers_filtered:
        print("\n[4/5] ğŸ¤– Gemini ê´€ë ¨ë„ ë¶„ì„ ë° í•­ëª©ë³„ ìš”ì•½ ì‹œì‘...")
        
        # --- âœ… [S2 í†µí•©] Notion DBì™€ ì¤‘ë³µ ì²´í¬ ---
        new_papers_to_analyze = [p for p in all_papers_filtered if p['title'].lower() not in existing_titles_lower]
        print(f"Notion DB ì¤‘ë³µ ì œì™¸ í›„, {len(new_papers_to_analyze)}ê°œì˜ ì‹ ê·œ ë…¼ë¬¸ì„ ë¶„ì„í•©ë‹ˆë‹¤.")

        for i, paper in enumerate(new_papers_to_analyze):
            print(f"--- ({i+1}/{len(new_papers_to_analyze)}) ğŸ”¬ Gemini ë¶„ì„ ì¤‘: {paper['title'][:60]}...")
            
            related_status, summary_parts = analyze_paper_with_gemini(paper)

            if related_status and summary_parts:
                analyzed_papers.append((paper, related_status, summary_parts))
                print(f"ğŸ‘ Gemini ë¶„ì„ ì™„ë£Œ! (ìƒíƒœ: {related_status})")
            else:
                print(f"ğŸ‘ Gemini ë¶„ì„ ì‹¤íŒ¨. ì´ ë…¼ë¬¸ì€ ë“±ë¡ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            time.sleep(1) # Gemini API ì†ë„ ì œí•œ

    print(f"\n[5/5] ğŸ“ Notion DBì— ìµœì¢… ë…¼ë¬¸ ë“±ë¡ ì‹œì‘...")
    if not analyzed_papers:
        print("âœ¨ ìƒˆë¡œ ì¶”ê°€í•  ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        # âœ… [S2 í†µí•©] Race Condition ë°©ì§€ë¥¼ ìœ„í•´ ìµœì¢… ëª©ë¡ì„ ë‹¤ì‹œ ê°€ì ¸ì˜µë‹ˆë‹¤.
        print("ğŸ”„ ìµœì¢… ì¤‘ë³µ ì²´í¬ë¥¼ ìœ„í•´ Notion DB ëª©ë¡ì„ ë‹¤ì‹œ ê°€ì ¸ì˜µë‹ˆë‹¤...")
        final_existing_titles_lower = {title.lower() for title in fetch_existing_titles()}
        
        final_papers_to_add = [
            (paper, status, parts)
            for paper, status, parts in analyzed_papers
            if paper['title'].lower() not in final_existing_titles_lower
        ]

        if not final_papers_to_add:
            print("âœ¨ ìµœì¢… ì¤‘ë³µ ì²´í¬ ê²°ê³¼, ìƒˆë¡œ ì¶”ê°€í•  ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            print(f"ì´ {len(final_papers_to_add)}ê°œì˜ ìƒˆë¡œìš´ ë…¼ë¬¸ì„ Notionì— ì¶”ê°€í•©ë‹ˆë‹¤.")
            for paper, status, parts in final_papers_to_add:
                add_to_notion(paper, status, parts)
                time.sleep(0.5) # Notion API ì†ë„ ì œí•œ

    print("\nğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

if __name__ == "__main__":
    main()
